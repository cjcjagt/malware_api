import numpy as np
from keras import Sequential
from keras.layers import Masking, LSTM, Dense
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.externals import joblib

files = {"Spyware", "Downloader", "Trojan", "Worms", "Adware", "Dropper", "Virus", "Backdoor"}
classes = {"Spyware": 0, "Downloader": 1, "Trojan": 2, "Worms": 3, "Adware": 4, "Dropper": 5, "Virus": 6, "Backdoor": 7}
dev_sample_percentage = 0.2
num_words = 270
batch_size=128
num_step = 100
class_num = 8
sequence_length = 10000


def dataPro():
    datas = []
    labels = []
    newline=[]
    for file in files:
        resData = open(file + ".txt", "a+")
        with open(file + "tem.txt", 'r') as f:
            for line in f.readlines():
                words = line.strip().split(" ")
                newline = [words[0]]
                for i in range(1, len(words)):
                    if len(newline) < 10000:
                        if words[i - 1] != words[i]:
                            newline.append(words[i])
                resData.write(" ".join(newline))
                resData.write('\n')
                newline.clear()
        resData.close()


def shuffleData(datas,labels,tokenflag=True):
    if(tokenflag):
        tokenizer = Tokenizer(num_words=num_words + 1, oov_token='UNK')
        tokenizer.fit_on_texts(datas)
        tokenizer.word_index = {e: i for e, i in tokenizer.word_index.items() if i <= num_words}
        tokenizer.word_index[tokenizer.oov_token] = 1
        print(len(tokenizer.word_index))
        joblib.dump(tokenizer, 'dataFile.pkl')

    tokenizer = joblib.load('dataFile.pkl')
    all_text_seq = tokenizer.texts_to_sequences(datas)
    all_text_test = pad_sequences(all_text_seq, maxlen=sequence_length,padding='pre', value=0)
    np.random.seed(100)
    shuffle_indices = np.random.permutation(np.arange(len(labels)))
    x_shuffled = np.array(all_text_test)[shuffle_indices.astype(int)]
    y_shuffled = np.array(labels)[shuffle_indices.astype(int)]

    # Split train/test set
    dev_sample_index = -1 * int(dev_sample_percentage * len(labels))
    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]
    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]
    del datas, labels, x_shuffled, y_shuffled, all_text_test
    return x_train, x_dev, y_train, y_dev, tokenizer


def lstm(em):
    model = Sequential()
    model.add(Masking(mask_value=0, input_shape=(sequence_length, em)))
    model.add(LSTM(50, dropout=0.2))
    # model.add(LSTM(100, return_sequences=False))
    model.add(Dense(8, activation='softmax'))
    model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
    return model


def train(x_train, y_train, x_val, y_val, em):
    model = lstm(em)
    model.fit(x_train, y_train, batch_size=batch_size, epochs=num_step, shuffle=True,
              validation_data=(x_val, y_val))
    loss, acc = model.evaluate(x_val, y_val, batch_size=batch_size)
    model.save('my_model.h5')
    print("\nTest score: %.3f, accuracy: %.3f" % (loss, acc))


def main():
    datas = []
    labels = []
    for file in files:
        with open(file + "def.txt", 'r') as f:
            # print("lins" + str(len(f.readlines())))
            for line in f.readlines():
                datas.append(line.strip("\n"))
                labels.append(classes[file])
    print(len(datas))
    print(len(labels))
    x_train, x_val, y_train, y_val, tokenizer = shuffleData(datas,labels,tokenflag=True)
    # vectorize(x_train,10000,tokenizer)
    x_train = x_train.reshape(len(x_train), sequence_length, 1)
    x_val = x_val.reshape(len(x_val), sequence_length, 1)
    train(x_train, y_train, x_val, y_val, 1)


if __name__ == '__main__':
    main()
