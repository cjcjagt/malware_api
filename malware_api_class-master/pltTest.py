import os
import gensim
import numpy as np
import sklearn
from keras import Sequential
from keras.layers import Masking, LSTM, Dense, Bidirectional
from keras.preprocessing.sequence import pad_sequences
from keras.callbacks import ReduceLROnPlateau, EarlyStopping
from keras import regularizers

files = {"Spyware", "Downloader", "Trojan", "Worms", "Adware", "Dropper", "Virus", "Backdoor"}
classes = {"Spyware": 0, "Downloader": 1, "Trojan": 2, "Worms": 3, "Adware": 4, "Dropper": 5, "Virus": 6, "Backdoor": 7}

batch_size = 64
num_step = 1000
class_num = 8
sequence_length = 7000
embedding = 100


class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            print(str(fname) + " is doing")
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()


def shuffleData(datas, labels, tokenflag=False):
    all_text_seq = []
    array = []
    if (tokenflag):
        sentences = MySentences('data/')  # a memory-friendly iterator
        model = gensim.models.Word2Vec(sentences, min_count=0)
        model.save('myWord2VecModel.model')

    gensim_model = gensim.models.Word2Vec.load('myWord2VecModel.model')
    print(len(gensim_model.wv.vocab))
    seq = []
    for line in datas:
        for word in line:
            if word in gensim_model:
                seq.append(gensim_model[word])
            else:
                seq.append(np.random.random((100)))
        array = np.asarray(seq, dtype='float32')
        all_text_seq.append(array.mean(axis=0))
        seq = []
    all_text_test = pad_sequences(all_text_seq, maxlen=sequence_length, padding='post', value=0)

    # label_one_hot = np.zeros((len(labels), class_num), dtype=np.int)
    # for i in range(len(labels)):
    #     label_one_hot[i][labels[i]] = 1
    np.random.seed(100)
    del all_text_seq
    shuffle_indices = np.random.permutation(np.arange(len(labels)))
    x_shuffled = np.array(all_text_test)[shuffle_indices.astype(int)]
    y_shuffled = np.array(labels)[shuffle_indices.astype(int)]

    # Split train/test set
    train, test, train_labels, test_labels = sklearn.model_selection.train_test_split(x_shuffled, y_shuffled,
                                                                                      test_size=.2,
                                                                                      random_state=42)
    train, val, train_labels, val_labels = sklearn.model_selection.train_test_split(train, train_labels, test_size=.1,
                                                                                    random_state=42)

    del datas, labels, x_shuffled, y_shuffled, all_text_test
    return train, val, train_labels, val_labels, test, test_labels


def lstm(embedding):
    model = Sequential()
    model.add(Masking(mask_value=0, input_shape=(sequence_length, embedding)))
    # model.add(Bidirectional(LSTM(50, dropout=0.3, return_sequences=True)))

    # model.add(LSTM(30, dropout=0.3, return_sequences=False))

    model.add(LSTM(128, activation='softsign', return_sequences=False))
    model.add(Dense(128, activation='relu', dropout=0.5))
    # model.add(LSTM(64, dropout=0.3, return_sequences=Falseï¼Œkernel_regularizer=regularizers.l1(0.01)))
    model.add(Dense(8, activation='softmax'))
    model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"],
                  callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0001)])
    return model


def main():
    datas = []
    labels = []
    for file in files:
        with open(file + "cut.txt", 'r') as f:
            for line in f.readlines():
                datas.append(line.strip("\n"))
                labels.append(classes[file])
    print(len(datas))
    print(len(labels))

    # print(len(datas[100]))
    # shuffleData(datas,labels,tokenflag=False)
    train, val, train_labels, val_labels, test, test_labels = shuffleData(datas, labels, tokenflag=False)
    x_train = train.reshape(len(train), sequence_length, 100)
    x_val = val.reshape(len(val), sequence_length, 100)
    print(x_train.shape)
    model = lstm(embedding)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, mode='auto')
    history = model.fit(x_train, train_labels, batch_size=batch_size, epochs=num_step, shuffle=True,
              validation_data=(x_val, val_labels), callbacks=[reduce_lr])

    loss, acc = model.evaluate(test, test_labels, batch_size=batch_size)
    model.save("my2VecModel.h5")
    print("\nTest score: %.3f, accuracy: %.3f" % (loss, acc))
    with open("./log/test.log", "a+") as f:
        f.write("\nval_loss-----------------------\n")
        f.write(str(history.history['val_loss']))
        f.write("\nval_acc-----------------------\n")
        f.write(str(history.history['val_accuracy']))
        f.write("\nloss--------------------------\n")
        f.write(str(history.history['loss']))
        f.write("\nacc---------------------------\n")
        f.write(str(history.history['accuracy']))

        f.write("\nTest loss:" + str(round(loss, 3)) + ", accuracy:" + str(round(acc, 3)))


if __name__ == '__main__':
    main()

