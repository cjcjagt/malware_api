import os
import gensim
import sklearn
import numpy as np
from keras import Sequential
from keras.layers import Masking, LSTM, Dense, Bidirectional, Dropout, Conv1D, GlobalMaxPooling1D, Activation, \
    Embedding, BatchNormalization
from keras.preprocessing.sequence import pad_sequences
from keras.utils import plot_model
from keras import regularizers

files = {"Spyware", "Downloader", "Trojan", "Worms", "Adware", "Dropper", "Virus", "Backdoor"}
classes = {"Spyware": 0, "Downloader": 1, "Trojan": 2, "Worms": 3, "Adware": 4, "Dropper": 5, "Virus": 6, "Backdoor": 7}

batch_size = 64
num_step = 500
class_num = 8
sequence_length = 5000
embedding = 50
dev_sample_percentage = 0.2
filters = 250
kernel_size = 4
hidden_dims = 128
path = "/content/drive/My Drive/Colab Notebooks/"


class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            print(str(fname) + " is doing")
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()


def shuffleData(datas, labels, tokenflag=False):
    all_text_seq = []
    seq = []
    num = 1
    if (tokenflag):
        sentences = MySentences('data/')  # a memory-friendly iterator
        model = gensim.models.Word2Vec(sentences, min_count=0, size=50)
        model.save('myWord2VecModel.model')

    gensim_model = gensim.models.Word2Vec.load(path + 'myWord2VecModel.model')
    word2idx = {"_PAD": 0}

    vocab_list = [(k, gensim_model.wv[k]) for k, v in gensim_model.wv.vocab.items()]

    embeddings_matrix = np.zeros((len(gensim_model.wv.vocab.items()) + 1, gensim_model.vector_size))
    for i in range(len(vocab_list)):
        word = vocab_list[i][0]
        word2idx[word] = i + 1
        embeddings_matrix[i + 1] = vocab_list[i][1]
    vocab_list = list(gensim_model.wv.vocab.keys())
    print(embeddings_matrix[0])
    word_index = {word: index for index, word in enumerate(vocab_list)}
    for line in datas:
        lineWords = line.split(" ")
        for index in range(len(lineWords) - 1):
            if num < sequence_length:
                if lineWords[index] in word_index and lineWords[index] != lineWords[index + 1]:
                    seq.append(word_index[lineWords[index]] + 1)
                    num += 1
                else:
                    pass
        all_text_seq.append(seq)
        seq = []
        num = 1

    all_text_test = pad_sequences(all_text_seq, maxlen=sequence_length, padding='post')
    # all_text_test = np.array(all_text_test)
    # np.savetxt("./data/all_data.txt", all_text_test)
    # # print(all_text_test)
    np.random.seed(1226)
    del all_text_seq
    shuffle_indices = np.random.permutation(np.arange(len(labels)))
    x_shuffled = np.array(all_text_test)[shuffle_indices.astype(int)]
    y_shuffled = np.array(labels)[shuffle_indices.astype(int)]

    dev_sample_index = -1 * int(dev_sample_percentage * len(labels))
    train, val = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]
    train_labels, val_labels = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]
    print(val_labels)
    del datas, labels, x_shuffled, y_shuffled, all_text_test
    np.savetxt(path + "data/cnntest_dataTwoConv7.txt", val, fmt="%d")
    np.savetxt(path + "data/cnnlabelTwoConv7.txt", val_labels, fmt="%d")
    print("saving complete")
    return train, val, train_labels, val_labels, embeddings_matrix


def text_cnn(embeddings_matrix):
    model = Sequential()
    # model.add(Masking(mask_value=0, input_shape=(sequence_length, embedding)))
    model.add(Embedding(
        input_dim=embeddings_matrix.shape[0],
        output_dim=embeddings_matrix.shape[1],
        # input_length=sequence_length,
        weights=[embeddings_matrix],
        trainable=False))

    model.add(Dropout(0.5))
    # we add a Convolution1D, which will learn filters
    # word group filters of size filter_length:
    model.add(Conv1D(filters,
                     kernel_size,
                     padding='valid',
                     activation='relu',

                     strides=1))
    # we use max pooling:
    # model.add(GlobalMaxPooling1D())
    # model.add(Dropout(0.3))
    # we add a Convolution1D, which will learn filters
    # word group filters of size filter_length:
    model.add(BatchNormalization())
    model.add(Conv1D(128,
                     kernel_size,
                     padding='valid',
                     activation='relu',

                     strides=1))
    # we use max pooling:
    model.add(BatchNormalization())
    model.add(GlobalMaxPooling1D())
    # We add a vanilla hidden layer:
    model.add(Dense(hidden_dims))
    model.add(Dropout(0.5))
    model.add(Activation('relu'))

    # We project onto a single unit output layer, and squash it with a sigmoid:
    model.add(Dense(8))
    model.add(Activation('softmax'))
    model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
    return model


def main():
    datas = []
    labels = []
    ind = 0
    for file in files:
        with open(path + "data/" + file + ".txt", 'r') as f:
            for line in f.readlines():
                datas.append(line.strip("\n"))
                labels.append(classes[file])

    train, val, train_labels, val_labels, embeddings_matrix = shuffleData(datas, labels, tokenflag=False)
    # x_train = train.reshape(len(train), sequence_length, embedding)
    # x_val = val.reshape(len(val), sequence_length, embedding )
    # print(x_train.shape)
    model = text_cnn(embeddings_matrix)
    # reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, mode='auto')
    history = model.fit(train, train_labels, batch_size=batch_size, epochs=num_step, shuffle=True,
                        validation_data=(val, val_labels))

    loss, acc = model.evaluate(val, val_labels, batch_size=batch_size)

    model.save(path + "CnnTextModelTwoConv7.h5")
    print("\nTest score: %.3f, accuracy: %.3f" % (loss, acc))
    with open(path + "log/CnnTextTwoConv7.log", "a+") as f:
        f.write("\nval_loss-----------------------\n")
        f.write(str(history.history['val_loss']))
        f.write("\nval_acc-----------------------\n")
        f.write(str(history.history['val_acc']))
        f.write("\nloss--------------------------\n")
        f.write(str(history.history['loss']))
        f.write("\nacc---------------------------\n")
        f.write(str(history.history['acc']))

        f.write("\nTest loss:" + str(round(loss, 3)) + ", accuracy:" + str(round(acc, 3)))
    model.summary()

    plot_model(model, to_file=path + 'CnnText_modelTwoConv7.jpg')


if __name__ == '__main__':
    main()
# Test score: 1.270, accuracy: 0.659