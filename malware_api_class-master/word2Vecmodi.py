import os
import gensim
import sklearn
import numpy as np
from keras import Sequential
from keras.layers import Masking, LSTM, Dense, Bidirectional, Embedding, Dropout, Flatten
from keras.preprocessing.sequence import pad_sequences
from keras.callbacks import ReduceLROnPlateau, EarlyStopping
from keras import regularizers

files = {"Spyware", "Downloader", "Trojan", "Worms", "Adware", "Dropper", "Virus", "Backdoor"}
classes = {"Spyware": 0, "Downloader": 1, "Trojan": 2, "Worms": 3, "Adware": 4, "Dropper": 5, "Virus": 6, "Backdoor": 7}

batch_size = 128
num_step = 1000
class_num = 8
sequence_length = 5000
embedding = 50
dev_sample_percentage = 0.2


class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            print(str(fname) + " is doing")
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()


def shuffleData(datas, labels, tokenflag=False):
    all_text_seq = []
    seq = []
    num = 1
    if (tokenflag):
        sentences = MySentences('data/')  # a memory-friendly iterator
        model = gensim.models.Word2Vec(sentences, min_count=0, size=50)
        model.save('myWord2VecModel.model')

    gensim_model = gensim.models.Word2Vec.load('myWord2VecModel.model')
    word2idx = {"_PAD": 0}  # 初始化 `[word : token]` 字典，后期 tokenize 语料库就是用该词典。

    vocab_list = [(k, gensim_model.wv[k]) for k, v in gensim_model.wv.vocab.items()]

    # 存储所有 word2vec 中所有向量的数组，留意其中多一位，词向量全为 0， 用于 padding
    embeddings_matrix = np.zeros((len(gensim_model.wv.vocab.items()) + 1, gensim_model.vector_size))
    for i in range(len(vocab_list)):
        word = vocab_list[i][0]
        word2idx[word] = i + 1
        embeddings_matrix[i + 1] = vocab_list[i][1]
    vocab_list = list(gensim_model.wv.vocab.keys())
    word_index = {word: index for index, word in enumerate(vocab_list)}
    for line in datas:
        lineWords = line.split(" ")
        for word in lineWords:
            if num < sequence_length:
                if word in word_index:
                    seq.append(word_index[word])
                else:
                    pass
                num += 1
        all_text_seq.append(seq)
        seq = []
        num =1
    all_text_test = pad_sequences(all_text_seq, maxlen=sequence_length, padding='post')
    # print(all_text_test)
    np.random.seed(100)
    del all_text_seq
    shuffle_indices = np.random.permutation(np.arange(len(labels)))
    x_shuffled = np.array(all_text_test)[shuffle_indices.astype(int)]
    y_shuffled = np.array(labels)[shuffle_indices.astype(int)]

    dev_sample_index = -1 * int(dev_sample_percentage * len(labels))
    train, val = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]
    train_labels, val_labels = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]
    del datas, labels, x_shuffled, y_shuffled, all_text_test
    return train, val, train_labels, val_labels,embeddings_matrix


def lstm(embeddings_matrix):
    # embedding_matrix = gensim_model.wv.vectors
    model = Sequential()
    model.add(Masking(input_shape=(sequence_length,)))

    # model.add(Flatten())
    model.add(Embedding(
        input_dim=embeddings_matrix.shape[0],
        output_dim=embeddings_matrix.shape[1],
        input_length=sequence_length,
        weights=[embeddings_matrix],
        trainable=False))

    model.add(Bidirectional(LSTM(128, recurrent_dropout=0.1)))
    model.add(Dropout(0.25))
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(8, activation='softmax'))
    model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
    return model


def main():
    datas = []
    labels = []
    for file in files:
        with open(file + "cut.txt", 'r') as f:
            for line in f.readlines():
                datas.append(line.strip("\n"))
                labels.append(classes[file])

    # print(len(datas[100]))
    shuffleData(datas,labels,tokenflag=False)
    train, val, train_labels, val_labels,embeddings_matrix = shuffleData(datas, labels, tokenflag=False)
    # train = train.reshape(len(train), sequence_length, 1)
    # val = val.reshape(len(val), sequence_length, 1)
    model = lstm(embeddings_matrix)
    early_stopping=EarlyStopping(monitor='val_loss', min_delta=0,
                              patience=0, verbose=0, mode='auto',
                              baseline=None, restore_best_weights=False)
    # reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, mode='auto')
    history = model.fit(train, train_labels, batch_size=batch_size, epochs=num_step, shuffle=True,
                        validation_data=(val, val_labels),callbacks=[early_stopping])

    loss, acc = model.evaluate(val, val_labels, batch_size=batch_size)

    model.save("my2VecModel.h5")
    print("\nTest score: %.3f, accuracy: %.3f" % (loss, acc))
    with open("./log/test.log", "a+") as f:
        f.write("\nval_loss-----------------------\n")
        f.write(str(history.history['val_loss']))
        f.write("\nval_acc-----------------------\n")
        f.write(str(history.history['val_accuracy']))
        f.write("\nloss--------------------------\n")
        f.write(str(history.history['loss']))
        f.write("\nacc---------------------------\n")
        f.write(str(history.history['accuracy']))

        f.write("\nTest loss:" + str(round(loss, 3)) + ", accuracy:" + str(round(acc, 3)))
    model.summary()
    from keras.utils import plot_model
    plot_model(model, to_file='graphics_model.jpg')


if __name__ == '__main__':
    main()

