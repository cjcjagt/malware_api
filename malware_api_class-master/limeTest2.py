import gensim
from keras.models import load_model
import numpy as np
from keras.utils import to_categorical
import lime
from keras_preprocessing.sequence import pad_sequences
from lime import lime_text
from lime.lime_text import LimeTextExplainer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import make_pipeline, Pipeline
import pandas as pd

class Token(BaseEstimator, TransformerMixin):
    def __init__(self, datas,sequence_length,wordModel):
        self.datas = datas
        self.sequence_length = sequence_length
        self.wordModel = wordModel

    def fit(self):
        return self

    def transform(self,x=None):
        vocab_list = list(self.wordModel.wv.vocab.keys())
        word_index = {word: index for index, word in enumerate(vocab_list)}
        seq = []
        all_text_seq = []
        num = 1
        for line in self.datas:
            words = line.split(" ")
            for index in range(len(words) - 1):
                if num < self.sequence_length:
                    if words[index] in word_index and words[index] != words[index + 1]:
                        seq.append(word_index[words[index]] + 1)
                        num += 1
                    else:
                        pass
            all_text_seq.append(seq)
            seq = []
            num = 1
        all_text_test = pad_sequences(all_text_seq, maxlen=self.sequence_length, padding='post')
        # print(all_text_test)
        return all_text_test

path ="D:/document/thesis/pro/graduate/malware_data/"
# path = "data/"
classes = {"Spyware", "Downloader", "Trojan", "Worms", "Adware", "Dropper", "Virus", "Backdoor"}
print('[Load model...]')
model = load_model(path+'CnnTextModel.h5', compile=False)

print('[Load data...]')
X = pd.np.genfromtxt(path+'repa.txt',dtype=np.str,delimiter='\t')
y = np.loadtxt(path+'label.txt', dtype=np.int32, delimiter=' ')
label = 6

print("X_dtype", X.dtype ) # int32
print("y_dtype", y.dtype  )# int32
# print("X_shape", X.shape)# (571151, 10)
print("y_shape", y.shape)  # (571151,)

# print("X_sample", X[1])  # [0 0 0 0 0 0 0 0 0 35]
print("y_sample", y[1])  # 0

# data_num = X.shape[0]
# print('Data_num:', data_num)  # 571151
# seq_len = X.shape[1]
# print('Sequence length:', seq_len) # 10
##
## Initialize One-hot-encoder
##
y_test = to_categorical(y)
y_test = y_test[:, np.newaxis, :]
print("y_test_sample", y_test[1])  # [1,0]
print("y_test_dtype", y_test.dtype)  # float32

# Padding sequence and prepare labelss

# print(len(x_test[5]))
# print("x_test_shape", x_test.shape)  # (571151, 10)
# print("y_test_shape", y_test.shape)  # (571151, 1, 2)
# print(type(x_test[0]))

gensim_model = gensim.models.Word2Vec.load(path+'myWord2VecModel.model')
sequence_length = 1131

c = Pipeline([('token',Token(X,sequence_length,gensim_model)),
              ('model',model)])
# s = Token(x_test,sequence_length,gensim_model).transform()
# print(s.shape)
explainer = LimeTextExplainer(class_names=classes)
idx = 0
# print(np.argmax(c.predict(X),axis=1))
print("start------------------------")
exp = explainer.explain_instance(X[idx], c.predict, num_features=20, labels=[2, 4])
print('Document id: %d' % idx)
print('Predicted class =', classes[np.argmax(c.predict(X[idx]))])
print ('Explanation for class %s' % classes[4])
print (''.join(map(str, exp.as_list(label=4))))
print('True class: %s' % classes[label])



