import os
import gensim
import numpy as np
from keras import Sequential
from keras.layers import Masking, LSTM, Dense
from keras.preprocessing.sequence import pad_sequences


files = {"Spyware", "Downloader", "Trojan", "Worms", "Adware", "Dropper", "Virus", "Backdoor"}
classes = {"Spyware": 0, "Downloader": 1, "Trojan": 2, "Worms": 3, "Adware": 4, "Dropper": 5, "Virus": 6, "Backdoor": 7}
dev_sample_percentage = 0.3
batch_size=128
num_step = 100
class_num = 8
sequence_length = 10000
embedding = 100

class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            print(str(fname)+"is doing")
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()

def shuffleData(datas,labels,tokenflag=True):
    all_text_seq =[]
    if(tokenflag):
        sentences = MySentences('data/') # a memory-friendly iterator
        model = gensim.models.Word2Vec(sentences,min_count=0)
        model.save('myWord2VecModel.model')

    gensim_model = gensim.models.Word2Vec.load('myWord2VecModel.model')
    print(len(gensim_model.wv.vocab))
    for line in datas:
        array = np.asarray([gensim_model[word] for word in line if word in gensim_model], dtype='float32')
        all_text_seq.append(array.mean(axis=0))
    print(len(all_text_seq))
    all_text_test = pad_sequences(all_text_seq, maxlen=sequence_length,padding='post', value=0)

    label_one_hot = np.zeros((len(labels), class_num), dtype=np.int)
    for i in range(len(labels)):
        label_one_hot[i][labels[i]] = 1
    np.random.seed(100)
    shuffle_indices = np.random.permutation(np.arange(len(labels)))
    x_shuffled = np.array(all_text_test)[shuffle_indices.astype(float)]
    y_shuffled = np.array(labels)[shuffle_indices.astype(int)]

    # Split train/test set
    dev_sample_index = -1 * int(dev_sample_percentage * len(labels))
    x_train, x_val = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]
    y_train, y_val = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]
    del datas, labels, x_shuffled, y_shuffled, all_text_test
    return x_train, x_val, y_train, y_val


def lstm(embedding):
    model = Sequential()
    model.add(Masking(mask_value=0, input_shape=(sequence_length, embedding)))
    model.add(LSTM(50, dropout=0.3, return_sequences=True))
    model.add(LSTM(100, return_sequences=False))
    model.add(Dense(8, activation='softmax'))
    model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
    return model


def main():
    datas = []
    labels = []
    for file in files:
        with open(file + "def.txt", 'r') as f:
            for line in f.readlines():
                datas.append(line.strip("\n"))
                labels.append(classes[file])
    print(len(datas))
    print(len(labels))
    x_train, x_val, y_train, y_val= shuffleData(datas,labels,tokenflag=True)
    x_train = x_train.reshape(len(x_train), sequence_length, 100)
    x_val = x_val.reshape(len(x_val), sequence_length, 100)
    print(x_train.shape)
    model = lstm(embedding)
    model.fit(x_train, y_train, batch_size=batch_size, epochs=num_step, shuffle=True,
                        validation_data=(x_val, y_val))

    loss, acc = model.evaluate(x_val, y_val, batch_size=batch_size)
    print("\nTest score: %.3f, accuracy: %.3f" % (loss, acc))


if __name__ == '__main__':
    main()
