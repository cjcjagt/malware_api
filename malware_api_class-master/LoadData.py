
import numpy as np
# import pandas as pd
# newline = []
# train_data = pd.read_table('all_analysis_data.txt', iterator=True, header=None)
#
# for data in train_data:
#     words = data.split(" ")
#     for i in range(1, len(words)):
#         if words[i - 1] != words[i]:
#             newline.append(words[i])
#     sequence = " ".join(newline)
# chunk = sequence  # 进行一些操作
# chunk.to_csv('output.csv', mode='a', header=False)

# import csv
#
# txt_file = "train.txt"
# csv_file = "train.csv"
# in_txt = csv.reader(open("all_analysis_data.txt", "r"), delimiter="\t",escapechar='\n')
# out_csv = csv.writer(open("output.csv", 'w'))
# out_csv.writerows(in_txt)

# import pandas as pd
# df = pd.read_csv("all_analysis_data.txt",
#                  error_bad_lines=False, chunksize=500,sep=" ", iterator=True)
# newline = []
# flag = 1
# for chunk in df:
#     for i in range(1, len(chunk)):
#             if chunk[i - 1] != chunk[i]:
#                 newline.append(chunk[i])
#             chunk = " ".join(newline)
#     chunk.to_csv('data_new.csv', mode='a', index=None, header=None)


import pandas as pd


# def load_data(file):
#     sequences = []
#     newline = []
#     index = 0
#     with open(file, "r") as f:
#         for line in f.readlines():
#             index +=1
#             words = line.split(" ")
#             for i in range(1, len(words)):
#                 if words[i - 1] != words[i]:
#                     newline.append(words[i])
#             sequences.append(" ".join(newline))
#             newline = []
#             if index%100==0:
#                 print("now is line index:"+str(index))
#                 test = pd.DataFrame(data=sequences)
#                 test.to_csv('data.csv', mode='a', index=None, header=None)
#                 sequences =[]
#         f.close()
#     test = pd.DataFrame(data=sequences)
#     test.to_csv('data.csv', mode='a', index=None, header=None)
#
#
# load_data("all_analysis_data.txt")

import pandas as pd


# def load_data(file):
#     sequences = []
#     newline = []
#     index = 0
#     flag =True
#     with open(file, "r") as f:
#         for line in f.readlines():
#             index += 1
#             words = line.strip("\n").split(" ")
#             for i in range(1, len(words)):
#                 if len(newline)<10000:
#                     if words[i - 1] != words[i]:
#                         newline.append(words[i])
#             sequences.append(newline)
#             newline = []
#             if index % 100 == 0:
#                 print("now is line index:" + str(index))
#                 test = pd.DataFrame(data=sequences)
#                 if flag:
#                     test.to_csv('train_data.csv', mode='a', index=None, header=range(10001))
#                     flag = False
#                 else:
#                     test.to_csv('train_data.csv', mode='a', index=None, header=None)
#                 sequences = []
#         f.close()
#     test = pd.DataFrame(data=sequences)
#     test.to_csv('train_data.csv', mode='a', index=None, header=None)
#
#
# load_data("all_analysis_data.txt")

import keras
# import pandas as pd
# from keras.preprocessing.text import Tokenizer
# from keras.preprocessing.sequence import pad_sequences
# import numpy as np
# data = pd.read_csv('data.csv')
# tokenizer = Tokenizer()
# tokenizer.fit_on_texts(data)
# sequences = tokenizer.texts_to_sequences(data)
# seq = pad_sequences(sequences, maxlen=1000)

# import pandas as pd
# data = pd.read_csv("sample_analysis_data.csv")
# print(data[:])

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import pandas as pd
# import jieba

def lstm():

    print("-----------------")

    data = pd.read_csv("train_data.csv")
    # print(data[0].)
    # data['text'] = data.comment.apply(lambda x: " ".join())
    tokenizer = Tokenizer(num_words=None)  # 分词MAX_NB_WORDS
    tokenizer.fit_on_texts(data)
    sequences = tokenizer.texts_to_sequences(data)  # 受num_words影响
    word_index = tokenizer.word_index  # 词_索引
    print('Found %s unique tokens.' % len(word_index))
    data = pad_sequences(sequences, maxlen=10000)  # 将长度不足 100 的新闻用 0 填充（在前端填充）
    print('Shape of data tensor:', data.shape)
    label = pd.read_csv("label .csv")
    print('Shape of label tensor:', label.shape)


lstm()




import pandas as pd


def load_data(file):
    sequences = []
    newline = []
    index = 0
    flag =True
    with open(file, "r") as f:
        token_index = {}
        for line in f.readlines():
            index += 1
            words = line.strip("\n").split(" ")
            for i in range(1, len(words)):
                if words[i] not in token_index:
                    token_index[words[i]] = len(token_index) + 1
                if len(newline)<10000:
                    if words[i - 1] != words[i]:
                        newline.append(token_index[words[i]])


load_data("all_analysis_data.txt")